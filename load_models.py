# -*- coding: utf-8 -*-
"""load_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x4sY3nlEdt5Axyedeffe_6pDhPLAlG9g
"""

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder
import re
from symspellpy import SymSpell, Verbosity
from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer
import torch
from sentence_transformers import SentenceTransformer, util
from llama_cpp import Llama
import torch
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
import os
from langdetect import detect

def load_dictionary(path):
  #autocorrect Dictionary
  sym=SymSpell(max_dictionary_edit_distance=2,prefix_length=10)
  sym.load_dictionary(path, term_index=0,count_index=1,separator="\t")
  return sym

def load_minillm(path):
  embeddings= HuggingFaceEmbeddings(model_name=path) #embeddings for Faiss
  intent_model = SentenceTransformer(path)
  return embeddings, intent_model

def load_crossencoder(path):
  reranker = CrossEncoder(path,device="cpu") #reranker for faiss retreived data
  return reranker

def load_faiss(path,embeddings):
  vector_db=FAISS.load_local( #FAISS
    path,
    embeddings,
    allow_dangerous_deserialization=True
)
  return vector_db

def load_qwen(path):
  threads=os.cpu_count()
  llm = Llama(
    model_path=path,
    n_threads=threads,   # adjust for your CPU
    n_ctx=4096,
    verbose=False
)
  return llm

def load_M2M100(path):
  tokenizer = M2M100Tokenizer.from_pretrained(path, local_files_only=True)
  model = M2M100ForConditionalGeneration.from_pretrained(path, local_files_only=True)
  model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
  model.to("cpu")
  model.eval()
  return model,tokenizer