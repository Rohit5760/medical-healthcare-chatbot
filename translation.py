# -*- coding: utf-8 -*-
"""translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XGR8eWKNYPRbPLahzc42yt6GqXBgMXVh
"""

from langdetect import detect

def detect_language(text):
  lang_map = {
    "as": "as",
    "bn": "bn",
    "gu": "gu",
    "hi": "hi",
    "kn": "kn",
    "ks": "ur",
    "ml": "ml",
    "mr": "mr",
    "ne": "ne",
    "or": "or",
    "pa": "pa",
    "sa": "hi",
    "sd": "ur",
    "ta": "ta",
    "te": "te",
    "ur": "ur",
    "en": "en",
}
  try:
    lang_code = detect(text)
  except:
    return "en"

  src_lang = lang_map.get(lang_code, None)

  if src_lang is None:
    return "en"

  return src_lang

def translate_to_english(text,tokenizer,model):
    src_lang = detect_language(text)
    if src_lang == "en":
        return text

    tokenizer.src_lang = src_lang
    encoded = tokenizer(text, return_tensors="pt")

    generated_tokens = model.generate(
        **encoded,
        forced_bos_token_id=tokenizer.get_lang_id("en"),
        max_length=300
    )

    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]

def translate_to_regional(original_text, response,tokenizer,model):
    tgt_lang = detect_language(original_text)
    if tgt_lang == "en":
        return response

    tokenizer.src_lang = "en"
    encoded = tokenizer(response, return_tensors="pt")

    generated_tokens = model.generate(
        **encoded,
        forced_bos_token_id=tokenizer.get_lang_id(tgt_lang),
        max_length=300
    )

    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]